{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baf8eba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /home/aakash/Development/Miimansa_assignment/CADEC.v2/cadec\n",
      "Text files: 1250 found\n",
      "✓ Gemini API initialized\n",
      "Loading embedding model (this may take a moment on first run)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1759772983.327931    9556 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Embedding model loaded\n",
      "\n",
      "======================================================================\n",
      "TASK 1: ENUMERATING DISTINCT ENTITIES\n",
      "======================================================================\n",
      "\n",
      "Processing files in: /home/aakash/Development/Miimansa_assignment/CADEC.v2/cadec/original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing annotation files: 100%|██████████| 1250/1250 [00:00<00:00, 27954.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  ADR        |  3400 unique entities\n",
      "             | Examples: ['trouble with walking/balance', 'scalp itching', 'shrinking muscels neck']\n",
      "  Drug       |   323 unique entities\n",
      "             | Examples: ['lecithin granules', 'ezetimbe', 'pravochol']\n",
      "  Finding    |   298 unique entities\n",
      "             | Examples: ['type2 diabetes', 'cramps in legs', 'generalized skin discoloration']\n",
      "  Disease    |   164 unique entities\n",
      "             | Examples: ['hyprochondria', 'rhabdomyolosis', 'lipid problem']\n",
      "  Symptom    |   148 unique entities\n",
      "             | Examples: ['elbow pain', 'severly restricted', 'chronic pain']\n",
      "\n",
      "✓ LangGraph workflow compiled\n",
      "\n",
      "======================================================================\n",
      "TASK 2-3: TESTING PIPELINE ON SAMPLE FILE\n",
      "======================================================================\n",
      "\n",
      "Processing: ARTHROTEC.1.txt\n",
      "Input length: 484 characters\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RESULTS\n",
      "======================================================================\n",
      "\n",
      "BIO Output (first 500 chars):\n",
      "I/O\n",
      "feel/O\n",
      "a/O\n",
      "bit/O\n",
      "drowsy/B-ADR\n",
      "&/O\n",
      "have/O\n",
      "a/O\n",
      "little/O\n",
      "blurred/B-ADR\n",
      "vision/I-ADR\n",
      ",/O\n",
      "so/O\n",
      "far/O\n",
      "no/O\n",
      "gastric/B-ADR\n",
      "problems/I-ADR\n",
      "./O\n",
      "I've/O\n",
      "been/O\n",
      "on/O\n",
      "Arthrotec/B-Drug\n",
      "50/O\n",
      "for/O\n",
      "over/O\n",
      "10/O\n",
      "years/O\n",
      "on/O\n",
      "and/O\n",
      "off/O\n",
      ",/O\n",
      "only/O\n",
      "taking/O\n",
      "it/O\n",
      "when/O\n",
      "I/O\n",
      "needed/O\n",
      "it/O\n",
      "./O\n",
      "Due/O\n",
      "to/O\n",
      "my/O\n",
      "arthritis/B-Disease\n",
      "getting/O\n",
      "progressively/O\n",
      "worse/O\n",
      ",/O\n",
      "to/O\n",
      "the/O\n",
      "point/O\n",
      "where/O\n",
      "I/O\n",
      "am/O\n",
      "in/O\n",
      "tears/O\n",
      "with/O\n",
      "the/O\n",
      "agony/B-Symptom\n",
      ",/O\n",
      "gp's/O\n",
      "started/O\n",
      "me/O\n",
      "on/O\n",
      "75/O\n",
      "twice/O\n",
      "a/O\n",
      "day/O\n",
      "and/O\n",
      "I/O\n",
      "have/O\n",
      "t...\n",
      "\n",
      "Predicted Entities (8):\n",
      "  ADR        |  13- 19 | drowsy\n",
      "  ADR        |  36- 50 | blurred vision\n",
      "  ADR        |  62- 78 | gastric problems\n",
      "  DRUG       |  93-102 | Arthrotec\n",
      "  DISEASE    | 179-188 | arthritis\n",
      "  SYMPTOM    | 260-265 | agony\n",
      "  SYMPTOM    | 412-417 | pains\n",
      "  ADR        | 448-453 | weird\n",
      "\n",
      "Ground Truth (8):\n",
      "  ADR        | bit drowsy\n",
      "  ADR        | little blurred vision\n",
      "  Drug       | Arthrotec\n",
      "  Disease    | arthritis\n",
      "  Symptom    | agony\n",
      "  ADR        | gastric problems\n",
      "  Symptom    | pains\n",
      "  ADR        | feel a bit weird\n",
      "\n",
      "Performance:\n",
      "  Precision: 0.7500\n",
      "  Recall:    0.7500\n",
      "  F1-Score:  0.7500\n",
      "  TP: 6, FP: 2, FN: 2\n",
      "\n",
      "======================================================================\n",
      "TASK 5: EVALUATING MULTIPLE FILES\n",
      "======================================================================\n",
      "\n",
      "Evaluating 50 random files\n",
      "\n",
      "Detailed output for first 2 files:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "FILE: LIPITOR.14.txt\n",
      "======================================================================\n",
      "Text length: 1508 chars\n",
      "\n",
      "\n",
      "BIO output length: 2359 chars\n",
      "Extracted entities: 20\n",
      "Ground truth: 24\n",
      "\n",
      "Predictions:\n",
      "  DRUG       | Lipitor\n",
      "  ADR        | extreme pain\n",
      "  ADR        | pain\n",
      "  ADR        | inner arms\n",
      "  ADR        | inflammed ligaments and bursae\n",
      "  ADR        | Nerve pain\n",
      "  ADR        | extreme burning\n",
      "  DRUG       | Liptor\n",
      "  ADR        | pains\n",
      "  ADR        | hurt\n",
      "  ADR        | inflammed bursae\n",
      "  DRUG       | statins\n",
      "  DRUG       | statin\n",
      "  DRUG       | Red Yeast Rice\n",
      "  DRUG       | COQ10\n",
      "  DRUG       | Vitamin C\n",
      "  ADR        | ankles on fire\n",
      "  DISEASE    | cholesterol\n",
      "  DISEASE    | elevated cholesterol\n",
      "  ADR        | symptoms\n",
      "\n",
      "Performance: P=0.500 R=0.417 F1=0.455\n",
      "\n",
      "======================================================================\n",
      "FILE: ARTHROTEC.141.txt\n",
      "======================================================================\n",
      "Text length: 473 chars\n",
      "\n",
      "\n",
      "BIO output length: 760 chars\n",
      "Extracted entities: 11\n",
      "Ground truth: 10\n",
      "\n",
      "Predictions:\n",
      "  ADR        | dizziness\n",
      "  ADR        | nausea\n",
      "  ADR        | stomach pain\n",
      "  ADR        | pain\n",
      "  ADR        | Elevated blood pressure\n",
      "  ADR        | heart palpitations\n",
      "  ADR        | dizzy\n",
      "  ADR        | sick\n",
      "  ADR        | Extreme gas\n",
      "  ADR        | acid reflux-type symptoms\n",
      "  ADR        | reflux\n",
      "\n",
      "Performance: P=0.727 R=0.800 F1=0.762\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Processing remaining files:\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   2%|▏         | 1/48 [00:01<01:24,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.528.txt                     | F1: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   4%|▍         | 2/48 [00:12<05:35,  7.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.464.txt                     | F1: 0.571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   6%|▋         | 3/48 [00:19<05:15,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.252.txt                     | F1: 0.400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   8%|▊         | 4/48 [00:26<05:01,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.816.txt                     | F1: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  10%|█         | 5/48 [00:45<08:11, 11.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.718.txt                     | F1: 0.800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  12%|█▎        | 6/48 [00:51<06:39,  9.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.583.txt                     | F1: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  15%|█▍        | 7/48 [01:17<10:04, 14.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.173.txt                     | F1: 0.364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  17%|█▋        | 8/48 [01:19<07:17, 10.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.975.txt                     | F1: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  19%|█▉        | 9/48 [01:32<07:25, 11.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.681.txt                     | F1: 0.545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  21%|██        | 10/48 [01:53<09:06, 14.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.401.txt                     | F1: 0.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  23%|██▎       | 11/48 [02:04<08:14, 13.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTHROTEC.24.txt                    | F1: 0.400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  25%|██▌       | 12/48 [02:15<07:40, 12.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.674.txt                     | F1: 0.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  27%|██▋       | 13/48 [02:29<07:33, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.649.txt                     | F1: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  29%|██▉       | 14/48 [02:40<07:08, 12.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.253.txt                     | F1: 0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  31%|███▏      | 15/48 [02:47<05:58, 10.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.817.txt                     | F1: 0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  33%|███▎      | 16/48 [02:58<05:45, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.144.txt                     | F1: 0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  35%|███▌      | 17/48 [03:08<05:31, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.892.txt                     | F1: 0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  38%|███▊      | 18/48 [03:21<05:38, 11.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.761.txt                     | F1: 0.571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  40%|███▉      | 19/48 [03:41<06:38, 13.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.824.txt                     | F1: 0.444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  42%|████▏     | 20/48 [03:55<06:29, 13.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.185.txt                     | F1: 0.429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  44%|████▍     | 21/48 [04:02<05:21, 11.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.55.txt                      | F1: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  46%|████▌     | 22/48 [04:16<05:24, 12.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.614.txt                     | F1: 0.235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  48%|████▊     | 23/48 [04:36<06:05, 14.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.568.txt                     | F1: 0.765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  50%|█████     | 24/48 [04:53<06:13, 15.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.316.txt                     | F1: 0.600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  52%|█████▏    | 25/48 [05:04<05:27, 14.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.782.txt                     | F1: 0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  54%|█████▍    | 26/48 [05:10<04:13, 11.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTHROTEC.118.txt                   | F1: 0.857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  56%|█████▋    | 27/48 [05:40<06:00, 17.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTHROTEC.135.txt                   | F1: 0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  58%|█████▊    | 28/48 [05:54<05:25, 16.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.382.txt                     | F1: 0.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  60%|██████    | 29/48 [06:08<04:53, 15.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.420.txt                     | F1: 0.889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  62%|██████▎   | 30/48 [06:23<04:38, 15.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOLTAREN-XR.3.txt                   | F1: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  65%|██████▍   | 31/48 [06:36<04:07, 14.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTHROTEC.56.txt                    | F1: 0.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  67%|██████▋   | 32/48 [06:42<03:15, 12.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.680.txt                     | F1: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  69%|██████▉   | 33/48 [06:50<02:42, 10.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.4.txt                       | F1: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  71%|███████   | 34/48 [06:56<02:09,  9.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.733.txt                     | F1: 0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  73%|███████▎  | 35/48 [07:07<02:09,  9.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.743.txt                     | F1: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  75%|███████▌  | 36/48 [07:24<02:24, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTHROTEC.117.txt                   | F1: 0.154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  77%|███████▋  | 37/48 [07:35<02:10, 11.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.206.txt                     | F1: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  79%|███████▉  | 38/48 [07:44<01:48, 10.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTHROTEC.102.txt                   | F1: 0.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  81%|████████▏ | 39/48 [07:52<01:30, 10.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTHROTEC.72.txt                    | F1: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  83%|████████▎ | 40/48 [08:10<01:39, 12.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.392.txt                     | F1: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  85%|████████▌ | 41/48 [08:15<01:10, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.525.txt                     | F1: 0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  88%|████████▊ | 42/48 [08:32<01:13, 12.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTHROTEC.50.txt                    | F1: 0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  90%|████████▉ | 43/48 [08:50<01:10, 14.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.786.txt                     | F1: 0.118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  92%|█████████▏| 44/48 [09:02<00:52, 13.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.272.txt                     | F1: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  94%|█████████▍| 45/48 [09:32<00:55, 18.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.800.txt                     | F1: 0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  96%|█████████▌| 46/48 [09:46<00:34, 17.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.735.txt                     | F1: 0.800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  98%|█████████▊| 47/48 [10:13<00:19, 19.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPITOR.17.txt                      | F1: 0.118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 48/48 [10:26<00:00, 13.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTHROTEC.20.txt                    | F1: 0.667\n",
      "\n",
      "======================================================================\n",
      "FINAL RESULTS\n",
      "======================================================================\n",
      "Files processed: 50/50\n",
      "\n",
      "Macro-Averaged Metrics:\n",
      "  Precision: 0.5787\n",
      "  Recall:    0.5971\n",
      "  F1-Score:  0.5731\n",
      "\n",
      "F1 Distribution:\n",
      "  Min:    0.0000\n",
      "  Q1:     0.4000\n",
      "  Median: 0.6667\n",
      "  Q3:     0.7619\n",
      "  Max:    1.0000\n",
      "\n",
      "Performance Breakdown:\n",
      "  Failures (F1=0):      4/50 (8.0%)\n",
      "  Poor (0 < F1 < 0.3):  5/50 (10.0%)\n",
      "  Moderate (0.3-0.6):   12/50 (24.0%)\n",
      "  Good (F1 >= 0.6):     29/50 (58.0%)\n",
      "\n",
      "======================================================================\n",
      "TASK 6: SNOMED-CT CODE LINKING\n",
      "======================================================================\n",
      "\n",
      "SCT Datastore:\n",
      "           sct_code                       sct_description label_type  \\\n",
      "0         271782001                                Drowsy        ADR   \n",
      "1         246636008                 Blurred vision - hazy        ADR   \n",
      "2         162076009  Excessive upper gastrointestinal gas        ADR   \n",
      "3  3384011000036100                             Arthrotec       Drug   \n",
      "4           3723001                             Arthritis    Disease   \n",
      "5         102498003                                 Agony    Symptom   \n",
      "6          76948002                           Severe pain    Symptom   \n",
      "7          22253000                                  Pain    Symptom   \n",
      "8         367391008                               Malaise        ADR   \n",
      "\n",
      "       ground_truth_text  \n",
      "0             bit drowsy  \n",
      "1  little blurred vision  \n",
      "2       gastric problems  \n",
      "3              Arthrotec  \n",
      "4              arthritis  \n",
      "5                  agony  \n",
      "6                  agony  \n",
      "7                  pains  \n",
      "8       feel a bit weird  \n",
      "\n",
      "Linking 4 ADR predictions to SNOMED-CT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "String Matching Results:\n",
      "----------------------------------------------------------------------\n",
      "  predicted_text                       best_sct_match  sct_code  match_score\n",
      "          drowsy                               Drowsy 271782001          100\n",
      "  blurred vision                Blurred vision - hazy 246636008           90\n",
      "gastric problems Excessive upper gastrointestinal gas 162076009           45\n",
      "           weird Excessive upper gastrointestinal gas 162076009           49\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Embedding Similarity Results:\n",
      "----------------------------------------------------------------------\n",
      "  predicted_text                       best_sct_match  sct_code  match_score\n",
      "          drowsy                               Drowsy 271782001     1.000000\n",
      "  blurred vision                Blurred vision - hazy 246636008     0.790501\n",
      "gastric problems Excessive upper gastrointestinal gas 162076009     0.652102\n",
      "           weird                               Drowsy 271782001     0.207762\n",
      "\n",
      "======================================================================\n",
      "COMPARISON\n",
      "======================================================================\n",
      "\n",
      "String Matching: Fast, handles typos, good for lexically similar terms\n",
      "Embedding Similarity: Captures semantic meaning, better for colloquial terms\n",
      "\n",
      "Recommendation: Use embeddings for patient→clinical terminology mapping\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TASK 4: ADR-SPECIFIC EVALUATION (vs MedDRA)\n",
      "======================================================================\n",
      "\n",
      "ADR-Only Performance (vs MedDRA):\n",
      "  ADR Predictions: 4\n",
      "  MedDRA Ground Truth: 4\n",
      "  Precision: 0.5000\n",
      "  Recall:    0.5000\n",
      "  F1-Score:  0.5000\n",
      "  TP: 2, FP: 2, FN: 2\n",
      "\n",
      "======================================================================\n",
      "DETAILED ERROR ANALYSIS\n",
      "======================================================================\n",
      "Analyzing: LIPITOR.733.txt\n",
      "\n",
      "======================================================================\n",
      "ERROR ANALYSIS: LIPITOR.733.txt\n",
      "======================================================================\n",
      "\n",
      "False Positives (2):\n",
      "  ADR        | tingling\n",
      "    Context: ...muscle pain, headache, tingling and sunny sensation on my face, gas. ...\n",
      "  ADR        | sunny sensation\n",
      "    Context: ...muscle pain, headache, tingling and sunny sensation on my face, gas. ...\n",
      "\n",
      "False Negatives (1) - Missed by system:\n",
      "  ADR        | tingling and sunny sensation on my face\n",
      "    Found in text: ...muscle pain, headache, tingling and sunny sensation on my face, gas. ...\n",
      "\n",
      "======================================================================\n",
      "PER-ENTITY-TYPE PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "Entity Type     Precision    Recall       F1-Score    \n",
      "-------------------------------------------------------\n",
      "ADR             0.6316       0.6261       0.6288      \n",
      "Drug            0.8772       0.6024       0.7143      \n",
      "Disease         0.1538       0.6000       0.2449      \n",
      "Symptom         0.2778       0.3125       0.2941      \n",
      "\n",
      "======================================================================\n",
      "PIPELINE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "APPROACH:\n",
      "- Zero-shot learning with Gemini 2.0 Flash\n",
      "- BIO tagging format with structured LangGraph pipeline\n",
      "- Relaxed Jaccard matching (>66% word overlap)\n",
      "\n",
      "STRENGTHS:\n",
      "✓ Modular architecture with LangGraph\n",
      "✓ Clear state management and error tracking\n",
      "✓ Context-aware ADR detection\n",
      "✓ Robust to format variations in LLM output\n",
      "\n",
      "LIMITATIONS:\n",
      "⚠ Zero-shot approach limits performance vs fine-tuned models\n",
      "⚠ Boundary detection for modifiers inconsistent\n",
      "⚠ Context-dependent labels (ADR vs Symptom) remain challenging\n",
      "⚠ Long texts may exceed model context window\n",
      "\n",
      "RECOMMENDATIONS FOR IMPROVEMENT:\n",
      "1. Fine-tune smaller model (e.g., BioBERT) on CADEC training set\n",
      "2. Add retry logic for malformed LLM outputs\n",
      "3. Implement active learning for uncertain cases\n",
      "4. Use ensemble of multiple models\n",
      "5. Add human-in-the-loop validation for low-confidence predictions\n",
      "\n",
      "\n",
      "OVERALL ASSESSMENT (F1=0.573): GOOD - Competitive with zero-shot approaches\n",
      "\n",
      "======================================================================\n",
      "PIPELINE COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS AND CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "from typing import TypedDict, Annotated, List, Dict\n",
    "import operator\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# LangGraph & LangChain\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Embeddings & Matching\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from thefuzz import process as fuzzy_process\n",
    "\n",
    "# For evaluation\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Data paths - reads from .env or uses defaults\n",
    "DATA_DIR = os.getenv('DATA_DIR', './cadec')\n",
    "TEXT_DIR = os.path.join(DATA_DIR, 'text')\n",
    "ORIGINAL_DIR = os.path.join(DATA_DIR, 'original')\n",
    "MEDDRA_DIR = os.path.join(DATA_DIR, 'meddra')\n",
    "SCT_DIR = os.path.join(DATA_DIR, 'sct')\n",
    "\n",
    "# Verify paths exist\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    raise FileNotFoundError(f\"Data directory not found: {DATA_DIR}\")\n",
    "\n",
    "print(f\"Data directory: {os.path.abspath(DATA_DIR)}\")\n",
    "print(f\"Text files: {len([f for f in os.listdir(TEXT_DIR) if f.endswith('.txt')])} found\")\n",
    "\n",
    "# ============================================================================\n",
    "# API INITIALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "# Get API key from environment\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "if not gemini_api_key:\n",
    "    raise ValueError(\n",
    "        \"GEMINI_API_KEY not found. Please:\\n\"\n",
    "        \"1. Create a .env file in your project root\\n\"\n",
    "        \"2. Add: GEMINI_API_KEY=your_key_here\\n\"\n",
    "        \"3. Get key from: https://aistudio.google.com/app/apikey\"\n",
    "    )\n",
    "\n",
    "# Initialize Gemini model\n",
    "try:\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        google_api_key=gemini_api_key,\n",
    "        temperature=0.1,\n",
    "        max_output_tokens=4096\n",
    "    )\n",
    "    print(\"✓ Gemini API initialized\")\n",
    "except Exception as e:\n",
    "    raise ConnectionError(f\"Failed to initialize Gemini API: {e}\")\n",
    "\n",
    "# Initialize embedding model (downloads on first run)\n",
    "print(\"Loading embedding model (this may take a moment on first run)...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"✓ Embedding model loaded\")\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 1: ENUMERATE DISTINCT ENTITIES\n",
    "# ============================================================================\n",
    "\n",
    "def enumerate_entities():\n",
    "    \"\"\"Parse all files to find distinct entities for each label type\"\"\"\n",
    "    distinct_entities = defaultdict(set)\n",
    "    line_regex = re.compile(r'^(T\\d+)\\t(\\w+)[\\s\\d;]+\\t(.+)$')\n",
    "    \n",
    "    print(f\"\\nProcessing files in: {ORIGINAL_DIR}\")\n",
    "    filenames = [f for f in os.listdir(ORIGINAL_DIR) if f.endswith('.ann')]\n",
    "    \n",
    "    for filename in tqdm(filenames, desc=\"Parsing annotation files\"):\n",
    "        filepath = os.path.join(ORIGINAL_DIR, filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('#'):\n",
    "                    continue\n",
    "                match = line_regex.match(line.strip())\n",
    "                if match:\n",
    "                    _, label, text = match.groups()\n",
    "                    normalized_text = text.strip().lower()\n",
    "                    if normalized_text:\n",
    "                        distinct_entities[label].add(normalized_text)\n",
    "    \n",
    "    return distinct_entities\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TASK 1: ENUMERATING DISTINCT ENTITIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "distinct_entities_by_label = enumerate_entities()\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for label, entities in distinct_entities_by_label.items():\n",
    "    print(f\"  {label:10} | {len(entities):5} unique entities\")\n",
    "    print(f\"             | Examples: {list(entities)[:3]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LANGGRAPH STATE DEFINITION\n",
    "# ============================================================================\n",
    "\n",
    "class NERState(TypedDict):\n",
    "    \"\"\"State for medical NER workflow\"\"\"\n",
    "    # Input\n",
    "    text: str\n",
    "    filename: str\n",
    "    \n",
    "    # LLM outputs\n",
    "    bio_output: str\n",
    "    llm_attempts: int\n",
    "    \n",
    "    # Parsed entities\n",
    "    raw_entities: List[tuple]\n",
    "    entities: List[Dict]\n",
    "    \n",
    "    # Ground truth & evaluation\n",
    "    ground_truth: List[Dict]\n",
    "    performance: Dict[str, float]\n",
    "    \n",
    "    # Error tracking\n",
    "    errors: Annotated[List[str], operator.add]\n",
    "\n",
    "# ============================================================================\n",
    "# PROMPT TEMPLATES\n",
    "# ============================================================================\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a medical NER expert specializing in extracting entities from patient forum posts.\n",
    "\n",
    "Your task is to label EVERY word using BIO format with these EXACT tags:\n",
    "- B-ADR, I-ADR: Adverse drug reactions (side effects from medication)\n",
    "- B-Drug, I-Drug: Medication names ONLY (not dosages)\n",
    "- B-Disease, I-Disease: Medical conditions being treated\n",
    "- B-Symptom, I-Symptom: Disease symptoms (NOT drug side effects)\n",
    "- O: Outside any entity\n",
    "\n",
    "CRITICAL FORMAT RULES:\n",
    "1. Output format: word/TAG word/TAG word/TAG\n",
    "2. NO punctuation in tags: \"pain/B-Symptom\" NOT \"pain/B-Symptom,\"\n",
    "3. NO invalid tags like B-DOSAGE, B-ACTIVITY, S-ADR\n",
    "4. Every word needs exactly ONE tag\n",
    "5. Punctuation gets O tag\n",
    "\n",
    "ENTITY DISTINCTION:\n",
    "- If symptom appears AFTER taking medication → ADR\n",
    "- If symptom is from the disease itself → Symptom\n",
    "- Numbers alone (50, 75) → O (not Drug)\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "Input: \"I feel dizzy and nauseous after taking Lipitor\"\n",
    "Output: I/O feel/B-ADR dizzy/I-ADR and/O nauseous/B-ADR after/O taking/O Lipitor/B-Drug\n",
    "\n",
    "Input: \"My arthritis pain is unbearable\"\n",
    "Output: My/O arthritis/B-Disease pain/B-Symptom is/O unbearable/O\n",
    "\n",
    "Input: \"Started Arthrotec 50 twice daily, no stomach issues so far\"\n",
    "Output: Started/O Arthrotec/B-Drug 50/O twice/O daily/O ,/O no/O stomach/B-ADR issues/I-ADR so/O far/O\n",
    "\n",
    "Input: \"Severe headache and muscle weakness from the medication\"\n",
    "Output: Severe/B-ADR headache/I-ADR and/O muscle/B-ADR weakness/I-ADR from/O the/O medication/O\"\"\"\n",
    "\n",
    "def create_user_prompt(text: str) -> str:\n",
    "    return f\"\"\"Label the following patient forum post in BIO format.\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "\n",
    "Output (word/TAG format only, no explanations):\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# LANGGRAPH NODE FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def generate_bio_labels(state: NERState) -> NERState:\n",
    "    \"\"\"Node: Generate BIO labels using Gemini\"\"\"\n",
    "    try:\n",
    "        messages = [\n",
    "            SystemMessage(content=SYSTEM_PROMPT),\n",
    "            HumanMessage(content=create_user_prompt(state['text']))\n",
    "        ]\n",
    "        \n",
    "        response = llm.invoke(messages)\n",
    "        bio_output = response.content.strip()\n",
    "        \n",
    "        state['bio_output'] = bio_output\n",
    "        state['llm_attempts'] = state.get('llm_attempts', 0) + 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        state['errors'].append(f\"LLM generation error: {str(e)}\")\n",
    "        state['bio_output'] = \"\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "def parse_bio_to_entities(state: NERState) -> NERState:\n",
    "    \"\"\"Node: Parse BIO output into entity tuples\"\"\"\n",
    "    bio_string = state['bio_output']\n",
    "    \n",
    "    if '/' not in bio_string:\n",
    "        state['errors'].append(\"No valid BIO tags found\")\n",
    "        state['raw_entities'] = []\n",
    "        return state\n",
    "    \n",
    "    VALID_TAGS = {'B-ADR', 'I-ADR', 'B-DRUG', 'I-DRUG', \n",
    "                  'B-DISEASE', 'I-DISEASE', 'B-SYMPTOM', 'I-SYMPTOM', 'O'}\n",
    "    \n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    entity_words = []\n",
    "    \n",
    "    for token in bio_string.split():\n",
    "        token = token.rstrip('.,;:!?/').strip()\n",
    "        \n",
    "        if not token or '/' not in token or token.startswith('/'):\n",
    "            continue\n",
    "        \n",
    "        parts = token.rsplit('/', 1)\n",
    "        if len(parts) != 2:\n",
    "            continue\n",
    "        \n",
    "        word, tag = parts\n",
    "        word = word.strip('.,;:!?/')\n",
    "        tag = tag.strip('.,;:!?/').upper().replace('DRUG', 'DRUG')\n",
    "        \n",
    "        if not word or word in ['/', ',', '.']:\n",
    "            continue\n",
    "        \n",
    "        if tag not in VALID_TAGS:\n",
    "            if current_entity and entity_words:\n",
    "                entities.append((current_entity, entity_words))\n",
    "            current_entity = None\n",
    "            entity_words = []\n",
    "            continue\n",
    "        \n",
    "        if tag.startswith('B-'):\n",
    "            if current_entity and entity_words:\n",
    "                entities.append((current_entity, entity_words))\n",
    "            current_entity = tag[2:]\n",
    "            entity_words = [word]\n",
    "            \n",
    "        elif tag.startswith('I-'):\n",
    "            label = tag[2:]\n",
    "            if current_entity == label:\n",
    "                entity_words.append(word)\n",
    "            else:\n",
    "                if current_entity and entity_words:\n",
    "                    entities.append((current_entity, entity_words))\n",
    "                current_entity = label\n",
    "                entity_words = [word]\n",
    "                \n",
    "        elif tag == 'O':\n",
    "            if current_entity and entity_words:\n",
    "                entities.append((current_entity, entity_words))\n",
    "            current_entity = None\n",
    "            entity_words = []\n",
    "    \n",
    "    if current_entity and entity_words:\n",
    "        entities.append((current_entity, entity_words))\n",
    "    \n",
    "    state['raw_entities'] = entities\n",
    "    \n",
    "    return state\n",
    "\n",
    "def map_to_text_spans(state: NERState) -> NERState:\n",
    "    \"\"\"Node: Map entities to character spans in original text\"\"\"\n",
    "    original_text = state['text']\n",
    "    original_lower = original_text.lower()\n",
    "    \n",
    "    final_entities = []\n",
    "    \n",
    "    for label, words in state['raw_entities']:\n",
    "        entity_text = ' '.join(words)\n",
    "        \n",
    "        if len(entity_text) <= 1 or (entity_text.isdigit() and len(entity_text) <= 2):\n",
    "            continue\n",
    "        \n",
    "        entity_lower = entity_text.lower()\n",
    "        start_idx = original_lower.find(entity_lower)\n",
    "        \n",
    "        if start_idx != -1:\n",
    "            actual_text = original_text[start_idx:start_idx + len(entity_text)]\n",
    "            final_entities.append({\n",
    "                'label': label,\n",
    "                'text': actual_text,\n",
    "                'start': start_idx,\n",
    "                'end': start_idx + len(entity_text)\n",
    "            })\n",
    "        else:\n",
    "            pattern = r'\\s+'.join(re.escape(w) for w in words)\n",
    "            match = re.search(pattern, original_lower, re.IGNORECASE)\n",
    "            if match:\n",
    "                actual_text = original_text[match.start():match.end()]\n",
    "                final_entities.append({\n",
    "                    'label': label,\n",
    "                    'text': actual_text,\n",
    "                    'start': match.start(),\n",
    "                    'end': match.end()\n",
    "                })\n",
    "    \n",
    "    state['entities'] = final_entities\n",
    "    \n",
    "    return state\n",
    "\n",
    "def apply_postprocessing(state: NERState) -> NERState:\n",
    "    \"\"\"Node: Context-aware ADR detection and deduplication\"\"\"\n",
    "    original_text = state['text']\n",
    "    entities = state['entities']\n",
    "    \n",
    "    for entity in entities:\n",
    "        if entity['label'] == 'SYMPTOM':\n",
    "            context_start = max(0, entity['start'] - 120)\n",
    "            context_end = min(len(original_text), entity['end'] + 120)\n",
    "            context = original_text[context_start:context_end].lower()\n",
    "            \n",
    "            drug_phrases = ['taking', 'take it', 'on', 'after', 'when on', \n",
    "                           'started me on', 'caused', 'from the']\n",
    "            \n",
    "            if any(phrase in context for phrase in drug_phrases):\n",
    "                has_drug = any(\n",
    "                    e['label'] == 'DRUG' and abs(e['start'] - entity['start']) < 150\n",
    "                    for e in entities\n",
    "                )\n",
    "                if has_drug:\n",
    "                    entity['label'] = 'ADR'\n",
    "    \n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for ent in entities:\n",
    "        key = (ent['label'], ent['text'].lower().strip())\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(ent)\n",
    "    \n",
    "    unique.sort(key=lambda x: x['start'])\n",
    "    state['entities'] = unique\n",
    "    \n",
    "    return state\n",
    "\n",
    "def load_ground_truth(state: NERState) -> NERState:\n",
    "    \"\"\"Node: Load ground truth annotations\"\"\"\n",
    "    ann_filename = state['filename'].replace('.txt', '.ann')\n",
    "    ann_path = os.path.join(ORIGINAL_DIR, ann_filename)\n",
    "    \n",
    "    entities = []\n",
    "    line_regex = re.compile(r'^(T\\d+)\\t(\\w+)\\s([\\d\\s;]+)\\t(.+)$')\n",
    "    \n",
    "    with open(ann_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            match = line_regex.match(line.strip())\n",
    "            if match:\n",
    "                _, label, _, text = match.groups()\n",
    "                entities.append({\n",
    "                    'label': label,\n",
    "                    'text': text.strip()\n",
    "                })\n",
    "    \n",
    "    state['ground_truth'] = entities\n",
    "    \n",
    "    return state\n",
    "\n",
    "def evaluate_performance(state: NERState) -> NERState:\n",
    "    \"\"\"Node: Calculate precision, recall, F1 with Jaccard matching\"\"\"\n",
    "    predictions = state['entities']\n",
    "    ground_truth = state['ground_truth']\n",
    "    \n",
    "    tp = 0\n",
    "    matched_gt = set()\n",
    "    \n",
    "    for pred in predictions:\n",
    "        pred_words = set(pred['text'].lower().split())\n",
    "        \n",
    "        for idx, gt in enumerate(ground_truth):\n",
    "            if idx in matched_gt:\n",
    "                continue\n",
    "            \n",
    "            if pred['label'].upper() != gt['label'].upper():\n",
    "                continue\n",
    "            \n",
    "            gt_words = set(gt['text'].lower().split())\n",
    "            \n",
    "            if pred_words and gt_words:\n",
    "                intersection = len(pred_words & gt_words)\n",
    "                union = len(pred_words | gt_words)\n",
    "                similarity = intersection / union if union > 0 else 0\n",
    "                \n",
    "                if similarity > 0.66:\n",
    "                    tp += 1\n",
    "                    matched_gt.add(idx)\n",
    "                    break\n",
    "    \n",
    "    fp = len(predictions) - tp\n",
    "    fn = len(ground_truth) - len(matched_gt)\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    state['performance'] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn\n",
    "    }\n",
    "    \n",
    "    return state\n",
    "\n",
    "# ============================================================================\n",
    "# BUILD LANGGRAPH WORKFLOW\n",
    "# ============================================================================\n",
    "\n",
    "def build_ner_graph():\n",
    "    \"\"\"Construct the LangGraph workflow\"\"\"\n",
    "    workflow = StateGraph(NERState)\n",
    "    \n",
    "    workflow.add_node(\"generate_bio\", generate_bio_labels)\n",
    "    workflow.add_node(\"parse_entities\", parse_bio_to_entities)\n",
    "    workflow.add_node(\"map_spans\", map_to_text_spans)\n",
    "    workflow.add_node(\"postprocess\", apply_postprocessing)\n",
    "    workflow.add_node(\"load_ground_truth\", load_ground_truth)\n",
    "    workflow.add_node(\"evaluate\", evaluate_performance)\n",
    "    \n",
    "    workflow.set_entry_point(\"generate_bio\")\n",
    "    workflow.add_edge(\"generate_bio\", \"parse_entities\")\n",
    "    workflow.add_edge(\"parse_entities\", \"map_spans\")\n",
    "    workflow.add_edge(\"map_spans\", \"postprocess\")\n",
    "    workflow.add_edge(\"postprocess\", \"load_ground_truth\")\n",
    "    workflow.add_edge(\"load_ground_truth\", \"evaluate\")\n",
    "    workflow.add_edge(\"evaluate\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "ner_graph = build_ner_graph()\n",
    "print(\"\\n✓ LangGraph workflow compiled\")\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 2-3: TEST ON SINGLE FILE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TASK 2-3: TESTING PIPELINE ON SAMPLE FILE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sample_filename = 'ARTHROTEC.1.txt'\n",
    "sample_path = os.path.join(TEXT_DIR, sample_filename)\n",
    "\n",
    "if not os.path.exists(sample_path):\n",
    "    print(f\"WARNING: {sample_filename} not found. Using first available file.\")\n",
    "    sample_filename = [f for f in os.listdir(TEXT_DIR) if f.endswith('.txt')][0]\n",
    "    sample_path = os.path.join(TEXT_DIR, sample_filename)\n",
    "\n",
    "with open(sample_path, 'r', encoding='utf-8') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "initial_state = {\n",
    "    'text': sample_text,\n",
    "    'filename': sample_filename,\n",
    "    'bio_output': '',\n",
    "    'llm_attempts': 0,\n",
    "    'raw_entities': [],\n",
    "    'entities': [],\n",
    "    'ground_truth': [],\n",
    "    'performance': {},\n",
    "    'errors': []\n",
    "}\n",
    "\n",
    "print(f\"\\nProcessing: {sample_filename}\")\n",
    "print(f\"Input length: {len(sample_text)} characters\\n\")\n",
    "\n",
    "result = ner_graph.invoke(initial_state)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nBIO Output (first 500 chars):\")\n",
    "print(result['bio_output'][:500] + \"...\" if len(result['bio_output']) > 500 else result['bio_output'])\n",
    "\n",
    "print(f\"\\nPredicted Entities ({len(result['entities'])}):\")\n",
    "for ent in result['entities']:\n",
    "    print(f\"  {ent['label']:10} | {ent['start']:3}-{ent['end']:3} | {ent['text']}\")\n",
    "\n",
    "print(f\"\\nGround Truth ({len(result['ground_truth'])}):\")\n",
    "for gt in result['ground_truth']:\n",
    "    print(f\"  {gt['label']:10} | {gt['text']}\")\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "perf = result['performance']\n",
    "print(f\"  Precision: {perf['precision']:.4f}\")\n",
    "print(f\"  Recall:    {perf['recall']:.4f}\")\n",
    "print(f\"  F1-Score:  {perf['f1']:.4f}\")\n",
    "print(f\"  TP: {perf['tp']}, FP: {perf['fp']}, FN: {perf['fn']}\")\n",
    "\n",
    "if result['errors']:\n",
    "    print(f\"\\nErrors: {result['errors']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 5: EVALUATE ON MULTIPLE FILES\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_file_with_graph(filename: str, verbose=False):\n",
    "    \"\"\"Evaluate single file using LangGraph\"\"\"\n",
    "    try:\n",
    "        with open(os.path.join(TEXT_DIR, filename), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        state = {\n",
    "            'text': text,\n",
    "            'filename': filename,\n",
    "            'bio_output': '',\n",
    "            'llm_attempts': 0,\n",
    "            'raw_entities': [],\n",
    "            'entities': [],\n",
    "            'ground_truth': [],\n",
    "            'performance': {},\n",
    "            'errors': []\n",
    "        }\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"FILE: {filename}\")\n",
    "            print(f\"{'='*70}\")\n",
    "            print(f\"Text length: {len(text)} chars\\n\")\n",
    "        \n",
    "        result = ner_graph.invoke(state)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nBIO output length: {len(result['bio_output'])} chars\")\n",
    "            print(f\"Extracted entities: {len(result['entities'])}\")\n",
    "            print(f\"Ground truth: {len(result['ground_truth'])}\")\n",
    "            print(f\"\\nPredictions:\")\n",
    "            for ent in result['entities']:\n",
    "                print(f\"  {ent['label']:10} | {ent['text']}\")\n",
    "            print(f\"\\nPerformance: P={result['performance']['precision']:.3f} \"\n",
    "                  f\"R={result['performance']['recall']:.3f} \"\n",
    "                  f\"F1={result['performance']['f1']:.3f}\")\n",
    "        \n",
    "        return result['performance']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TASK 5: EVALUATING MULTIPLE FILES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_files = [f for f in os.listdir(TEXT_DIR) if f.endswith('.txt')]\n",
    "random.seed(42)\n",
    "random_files = random.sample(all_files, min(50, len(all_files)))\n",
    "\n",
    "print(f\"\\nEvaluating {len(random_files)} random files\\n\")\n",
    "\n",
    "# Verbose for first 2\n",
    "scores = []\n",
    "print(\"Detailed output for first 2 files:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for f in random_files[:2]:\n",
    "    perf = evaluate_file_with_graph(f, verbose=True)\n",
    "    if perf:\n",
    "        scores.append(perf)\n",
    "\n",
    "# Summary for rest\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Processing remaining files:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for f in tqdm(random_files[2:], desc=\"Evaluating\"):\n",
    "    perf = evaluate_file_with_graph(f, verbose=False)\n",
    "    if perf:\n",
    "        scores.append(perf)\n",
    "        print(f\"{f:35} | F1: {perf['f1']:.3f}\")\n",
    "\n",
    "# Final statistics\n",
    "if scores:\n",
    "    avg_p = sum(s['precision'] for s in scores) / len(scores)\n",
    "    avg_r = sum(s['recall'] for s in scores) / len(scores)\n",
    "    avg_f1 = sum(s['f1'] for s in scores) / len(scores)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Files processed: {len(scores)}/{len(random_files)}\")\n",
    "    print(f\"\\nMacro-Averaged Metrics:\")\n",
    "    print(f\"  Precision: {avg_p:.4f}\")\n",
    "    print(f\"  Recall:    {avg_r:.4f}\")\n",
    "    print(f\"  F1-Score:  {avg_f1:.4f}\")\n",
    "    \n",
    "    f1_scores = [s['f1'] for s in scores]\n",
    "    print(f\"\\nF1 Distribution:\")\n",
    "    print(f\"  Min:    {min(f1_scores):.4f}\")\n",
    "    print(f\"  Q1:     {sorted(f1_scores)[len(f1_scores)//4]:.4f}\")\n",
    "    print(f\"  Median: {sorted(f1_scores)[len(f1_scores)//2]:.4f}\")\n",
    "    print(f\"  Q3:     {sorted(f1_scores)[3*len(f1_scores)//4]:.4f}\")\n",
    "    print(f\"  Max:    {max(f1_scores):.4f}\")\n",
    "    \n",
    "    zero_count = sum(1 for s in f1_scores if s == 0.0)\n",
    "    low_count = sum(1 for s in f1_scores if 0 < s < 0.3)\n",
    "    mid_count = sum(1 for s in f1_scores if 0.3 <= s < 0.6)\n",
    "    high_count = sum(1 for s in f1_scores if s >= 0.6)\n",
    "    \n",
    "    print(f\"\\nPerformance Breakdown:\")\n",
    "    print(f\"  Failures (F1=0):      {zero_count}/{len(scores)} ({zero_count/len(scores)*100:.1f}%)\")\n",
    "    print(f\"  Poor (0 < F1 < 0.3):  {low_count}/{len(scores)} ({low_count/len(scores)*100:.1f}%)\")\n",
    "    print(f\"  Moderate (0.3-0.6):   {mid_count}/{len(scores)} ({mid_count/len(scores)*100:.1f}%)\")\n",
    "    print(f\"  Good (F1 >= 0.6):     {high_count}/{len(scores)} ({high_count/len(scores)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 6: SNOMED-CT LINKING\n",
    "# ============================================================================\n",
    "\n",
    "def create_sct_datastore(ann_filename: str):\n",
    "    \"\"\"Combine original and SCT annotations\"\"\"\n",
    "    sct_filepath = os.path.join(SCT_DIR, ann_filename)\n",
    "    original_filepath = os.path.join(ORIGINAL_DIR, ann_filename)\n",
    "    \n",
    "    text_to_label = {}\n",
    "    line_regex_orig = re.compile(r'^(T\\d+)\\t(\\w+)[\\s\\d;]+\\t(.+)$')\n",
    "    \n",
    "    with open(original_filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            match = line_regex_orig.match(line.strip())\n",
    "            if match:\n",
    "                text_to_label[match.group(3).strip()] = match.group(2)\n",
    "    \n",
    "    sct_data = []\n",
    "    line_regex_sct = re.compile(r'^(TT\\d+)\\t(.*?)\\t(.*?)$')\n",
    "    \n",
    "    with open(sct_filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            match = line_regex_sct.match(line.strip())\n",
    "            if match:\n",
    "                _, codes_and_descs_raw, text_and_spans = match.groups()\n",
    "                ground_truth_text = re.split(r'\\d+\\s\\d+', text_and_spans)[-1].strip()\n",
    "                label = text_to_label.get(ground_truth_text, 'Unknown')\n",
    "                \n",
    "                codes_and_descs = re.findall(r'(\\d+)\\s*\\|\\s*(.*?)\\s*(?=\\||$)', codes_and_descs_raw)\n",
    "                \n",
    "                for code, desc in codes_and_descs:\n",
    "                    sct_data.append({\n",
    "                        'sct_code': code.strip(),\n",
    "                        'sct_description': desc.strip(),\n",
    "                        'label_type': label,\n",
    "                        'ground_truth_text': ground_truth_text\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(sct_data)\n",
    "\n",
    "def link_with_string_matching(predicted_adrs, sct_datastore):\n",
    "    \"\"\"Link ADRs using fuzzy string matching\"\"\"\n",
    "    sct_adr_df = sct_datastore[sct_datastore['label_type'] == 'ADR']\n",
    "    sct_descriptions = sct_adr_df['sct_description'].unique().tolist()\n",
    "    \n",
    "    if not predicted_adrs or not sct_descriptions:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    results = []\n",
    "    for adr in predicted_adrs:\n",
    "        best_match, score = fuzzy_process.extractOne(adr['text'], sct_descriptions)\n",
    "        matched_row = sct_adr_df[sct_adr_df['sct_description'] == best_match].iloc[0]\n",
    "        \n",
    "        results.append({\n",
    "            'predicted_text': adr['text'],\n",
    "            'best_sct_match': best_match,\n",
    "            'sct_code': matched_row['sct_code'],\n",
    "            'match_score': score\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def link_with_embeddings(predicted_adrs, sct_datastore):\n",
    "    \"\"\"Link ADRs using semantic embeddings\"\"\"\n",
    "    sct_adr_df = sct_datastore[sct_datastore['label_type'] == 'ADR'].drop_duplicates(subset=['sct_description'])\n",
    "    sct_descriptions = sct_adr_df['sct_description'].tolist()\n",
    "    \n",
    "    if not predicted_adrs or not sct_descriptions:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    pred_texts = [adr['text'] for adr in predicted_adrs]\n",
    "    pred_embeddings = embedding_model.encode(pred_texts, convert_to_tensor=True)\n",
    "    sct_embeddings = embedding_model.encode(sct_descriptions, convert_to_tensor=True)\n",
    "    \n",
    "    similarities = util.cos_sim(pred_embeddings, sct_embeddings)\n",
    "    \n",
    "    results = []\n",
    "    for i, adr in enumerate(predicted_adrs):\n",
    "        best_idx = similarities[i].argmax().item()\n",
    "        \n",
    "        results.append({\n",
    "            'predicted_text': adr['text'],\n",
    "            'best_sct_match': sct_descriptions[best_idx],\n",
    "            'sct_code': sct_adr_df.iloc[best_idx]['sct_code'],\n",
    "            'match_score': similarities[i][best_idx].item()\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TASK 6: SNOMED-CT CODE LINKING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sample_ann = sample_filename.replace('.txt', '.ann')\n",
    "sct_df = create_sct_datastore(sample_ann)\n",
    "\n",
    "print(f\"\\nSCT Datastore:\")\n",
    "print(sct_df.head(10))\n",
    "\n",
    "adr_predictions = [e for e in result['entities'] if e['label'] == 'ADR']\n",
    "\n",
    "if adr_predictions:\n",
    "    print(f\"\\nLinking {len(adr_predictions)} ADR predictions to SNOMED-CT...\")\n",
    "    \n",
    "    string_results = link_with_string_matching(adr_predictions, sct_df)\n",
    "    embedding_results = link_with_embeddings(adr_predictions, sct_df)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"String Matching Results:\")\n",
    "    print(\"-\"*70)\n",
    "    print(string_results.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"Embedding Similarity Results:\")\n",
    "    print(\"-\"*70)\n",
    "    print(embedding_results.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\"\"\n",
    "String Matching: Fast, handles typos, good for lexically similar terms\n",
    "Embedding Similarity: Captures semantic meaning, better for colloquial terms\n",
    "\n",
    "Recommendation: Use embeddings for patient→clinical terminology mapping\n",
    "\"\"\")\n",
    "else:\n",
    "    print(\"\\nNo ADR predictions found for linking demonstration.\")\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 4: ADR-SPECIFIC EVALUATION (MedDRA)\n",
    "# ============================================================================\n",
    "\n",
    "def parse_meddra_truth(filepath: str):\n",
    "    \"\"\"Parse MedDRA ground truth file\"\"\"\n",
    "    entities = []\n",
    "    line_regex = re.compile(r'^(TT\\d+)\\t\\d+\\s[\\d\\s]+\\t(.+)$')\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            match = line_regex.match(line.strip())\n",
    "            if match:\n",
    "                _, text = match.groups()\n",
    "                entities.append({'label': 'ADR', 'text': text.strip()})\n",
    "    return entities\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TASK 4: ADR-SPECIFIC EVALUATION (vs MedDRA)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "meddra_ann = sample_filename.replace('.txt', '.ann')\n",
    "meddra_path = os.path.join(MEDDRA_DIR, meddra_ann)\n",
    "\n",
    "if os.path.exists(meddra_path):\n",
    "    meddra_ground_truth = parse_meddra_truth(meddra_path)\n",
    "    \n",
    "    # Filter only ADR predictions\n",
    "    adr_only_preds = [e for e in result['entities'] if e['label'] == 'ADR']\n",
    "    \n",
    "    # Calculate ADR-specific performance\n",
    "    tp = 0\n",
    "    matched_gt = set()\n",
    "    \n",
    "    for pred in adr_only_preds:\n",
    "        pred_words = set(pred['text'].lower().split())\n",
    "        \n",
    "        for idx, gt in enumerate(meddra_ground_truth):\n",
    "            if idx in matched_gt:\n",
    "                continue\n",
    "            \n",
    "            gt_words = set(gt['text'].lower().split())\n",
    "            \n",
    "            if pred_words and gt_words:\n",
    "                intersection = len(pred_words & gt_words)\n",
    "                union = len(pred_words | gt_words)\n",
    "                similarity = intersection / union if union > 0 else 0\n",
    "                \n",
    "                if similarity > 0.66:\n",
    "                    tp += 1\n",
    "                    matched_gt.add(idx)\n",
    "                    break\n",
    "    \n",
    "    fp = len(adr_only_preds) - tp\n",
    "    fn = len(meddra_ground_truth) - len(matched_gt)\n",
    "    \n",
    "    adr_precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    adr_recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    adr_f1 = 2 * adr_precision * adr_recall / (adr_precision + adr_recall) if (adr_precision + adr_recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nADR-Only Performance (vs MedDRA):\")\n",
    "    print(f\"  ADR Predictions: {len(adr_only_preds)}\")\n",
    "    print(f\"  MedDRA Ground Truth: {len(meddra_ground_truth)}\")\n",
    "    print(f\"  Precision: {adr_precision:.4f}\")\n",
    "    print(f\"  Recall:    {adr_recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {adr_f1:.4f}\")\n",
    "    print(f\"  TP: {tp}, FP: {fp}, FN: {fn}\")\n",
    "else:\n",
    "    print(f\"\\nMedDRA file not found: {meddra_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS TO FILE\n",
    "# ============================================================================\n",
    "\n",
    "def save_results_to_csv(all_scores, output_filename='ner_results.csv'):\n",
    "    \"\"\"Save evaluation results to CSV\"\"\"\n",
    "    results_df = pd.DataFrame(all_scores)\n",
    "    results_df.to_csv(output_filename, index=False)\n",
    "    print(f\"\\n✓ Results saved to {output_filename}\")\n",
    "\n",
    "# Uncomment to save results from the 10-file evaluation:\n",
    "# save_results_to_csv(scores)\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION & ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_errors(filename: str):\n",
    "    \"\"\"Detailed error analysis for a specific file\"\"\"\n",
    "    with open(os.path.join(TEXT_DIR, filename), 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    state = {\n",
    "        'text': text,\n",
    "        'filename': filename,\n",
    "        'bio_output': '',\n",
    "        'llm_attempts': 0,\n",
    "        'raw_entities': [],\n",
    "        'entities': [],\n",
    "        'ground_truth': [],\n",
    "        'performance': {},\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    result = ner_graph.invoke(state)\n",
    "    \n",
    "    predictions = result['entities']\n",
    "    ground_truth = result['ground_truth']\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ERROR ANALYSIS: {filename}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Find false positives\n",
    "    matched_preds = set()\n",
    "    for pred in predictions:\n",
    "        pred_words = set(pred['text'].lower().split())\n",
    "        for gt in ground_truth:\n",
    "            if pred['label'].upper() == gt['label'].upper():\n",
    "                gt_words = set(gt['text'].lower().split())\n",
    "                if pred_words and gt_words:\n",
    "                    similarity = len(pred_words & gt_words) / len(pred_words | gt_words)\n",
    "                    if similarity > 0.66:\n",
    "                        matched_preds.add(pred['text'].lower())\n",
    "                        break\n",
    "    \n",
    "    false_positives = [p for p in predictions if p['text'].lower() not in matched_preds]\n",
    "    \n",
    "    if false_positives:\n",
    "        print(f\"\\nFalse Positives ({len(false_positives)}):\")\n",
    "        for fp in false_positives:\n",
    "            print(f\"  {fp['label']:10} | {fp['text']}\")\n",
    "            # Show context\n",
    "            start = max(0, fp['start'] - 50)\n",
    "            end = min(len(text), fp['end'] + 50)\n",
    "            context = text[start:end].replace('\\n', ' ')\n",
    "            print(f\"    Context: ...{context}...\")\n",
    "    \n",
    "    # Find false negatives\n",
    "    matched_gt = set()\n",
    "    for gt in ground_truth:\n",
    "        gt_words = set(gt['text'].lower().split())\n",
    "        for pred in predictions:\n",
    "            if pred['label'].upper() == gt['label'].upper():\n",
    "                pred_words = set(pred['text'].lower().split())\n",
    "                if pred_words and gt_words:\n",
    "                    similarity = len(pred_words & gt_words) / len(pred_words | gt_words)\n",
    "                    if similarity > 0.66:\n",
    "                        matched_gt.add(gt['text'].lower())\n",
    "                        break\n",
    "    \n",
    "    false_negatives = [g for g in ground_truth if g['text'].lower() not in matched_gt]\n",
    "    \n",
    "    if false_negatives:\n",
    "        print(f\"\\nFalse Negatives ({len(false_negatives)}) - Missed by system:\")\n",
    "        for fn in false_negatives:\n",
    "            print(f\"  {fn['label']:10} | {fn['text']}\")\n",
    "            # Try to find in text\n",
    "            if fn['text'].lower() in text.lower():\n",
    "                idx = text.lower().find(fn['text'].lower())\n",
    "                start = max(0, idx - 50)\n",
    "                end = min(len(text), idx + len(fn['text']) + 50)\n",
    "                context = text[start:end].replace('\\n', ' ')\n",
    "                print(f\"    Found in text: ...{context}...\")\n",
    "            else:\n",
    "                print(f\"    Text not found verbatim in input\")\n",
    "    \n",
    "    return {\n",
    "        'false_positives': false_positives,\n",
    "        'false_negatives': false_negatives\n",
    "    }\n",
    "\n",
    "# Example: Analyze errors for a specific file\n",
    "if scores and len(scores) > 0:\n",
    "    # Find a file with moderate performance for interesting analysis\n",
    "    mid_performing_idx = len(scores) // 2\n",
    "    sorted_files = sorted(zip(random_files, scores), key=lambda x: x[1]['f1'])\n",
    "    mid_file = sorted_files[mid_performing_idx][0]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DETAILED ERROR ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Analyzing: {mid_file}\")\n",
    "    \n",
    "    error_analysis = analyze_errors(mid_file)\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS BY ENTITY TYPE\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_per_entity_metrics(all_results):\n",
    "    \"\"\"Calculate P/R/F1 for each entity type separately\"\"\"\n",
    "    entity_stats = defaultdict(lambda: {'tp': 0, 'fp': 0, 'fn': 0})\n",
    "    \n",
    "    for filename, performance in zip(random_files, scores):\n",
    "        with open(os.path.join(TEXT_DIR, filename), 'r') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        state = {\n",
    "            'text': text,\n",
    "            'filename': filename,\n",
    "            'bio_output': '',\n",
    "            'llm_attempts': 0,\n",
    "            'raw_entities': [],\n",
    "            'entities': [],\n",
    "            'ground_truth': [],\n",
    "            'performance': {},\n",
    "            'errors': []\n",
    "        }\n",
    "        \n",
    "        result = ner_graph.invoke(state)\n",
    "        \n",
    "        for entity_type in ['ADR', 'Drug', 'Disease', 'Symptom']:\n",
    "            preds = [e for e in result['entities'] if e['label'].upper() == entity_type.upper()]\n",
    "            gts = [e for e in result['ground_truth'] if e['label'].upper() == entity_type.upper()]\n",
    "            \n",
    "            matched = set()\n",
    "            for pred in preds:\n",
    "                pred_words = set(pred['text'].lower().split())\n",
    "                for idx, gt in enumerate(gts):\n",
    "                    if idx in matched:\n",
    "                        continue\n",
    "                    gt_words = set(gt['text'].lower().split())\n",
    "                    if pred_words and gt_words:\n",
    "                        similarity = len(pred_words & gt_words) / len(pred_words | gt_words)\n",
    "                        if similarity > 0.66:\n",
    "                            entity_stats[entity_type]['tp'] += 1\n",
    "                            matched.add(idx)\n",
    "                            break\n",
    "                else:\n",
    "                    entity_stats[entity_type]['fp'] += 1\n",
    "            \n",
    "            entity_stats[entity_type]['fn'] += len(gts) - len(matched)\n",
    "    \n",
    "    return entity_stats\n",
    "\n",
    "if scores:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PER-ENTITY-TYPE PERFORMANCE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    entity_metrics = calculate_per_entity_metrics(scores)\n",
    "    \n",
    "    print(f\"\\n{'Entity Type':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for entity_type in ['ADR', 'Drug', 'Disease', 'Symptom']:\n",
    "        stats = entity_metrics[entity_type]\n",
    "        tp, fp, fn = stats['tp'], stats['fp'], stats['fn']\n",
    "        \n",
    "        p = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        r = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0\n",
    "        \n",
    "        print(f\"{entity_type:<15} {p:<12.4f} {r:<12.4f} {f1:<12.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY & RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PIPELINE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "APPROACH:\n",
    "- Zero-shot learning with Gemini 2.0 Flash\n",
    "- BIO tagging format with structured LangGraph pipeline\n",
    "- Relaxed Jaccard matching (>66% word overlap)\n",
    "\n",
    "STRENGTHS:\n",
    "✓ Modular architecture with LangGraph\n",
    "✓ Clear state management and error tracking\n",
    "✓ Context-aware ADR detection\n",
    "✓ Robust to format variations in LLM output\n",
    "\n",
    "LIMITATIONS:\n",
    "⚠ Zero-shot approach limits performance vs fine-tuned models\n",
    "⚠ Boundary detection for modifiers inconsistent\n",
    "⚠ Context-dependent labels (ADR vs Symptom) remain challenging\n",
    "⚠ Long texts may exceed model context window\n",
    "\n",
    "RECOMMENDATIONS FOR IMPROVEMENT:\n",
    "1. Fine-tune smaller model (e.g., BioBERT) on CADEC training set\n",
    "2. Add retry logic for malformed LLM outputs\n",
    "3. Implement active learning for uncertain cases\n",
    "4. Use ensemble of multiple models\n",
    "5. Add human-in-the-loop validation for low-confidence predictions\n",
    "\"\"\")\n",
    "\n",
    "if scores and len(scores) > 0:\n",
    "    avg_f1 = sum(s['f1'] for s in scores) / len(scores)\n",
    "    \n",
    "    if avg_f1 >= 0.6:\n",
    "        assessment = \"EXCELLENT - Outperforms typical zero-shot baselines\"\n",
    "    elif avg_f1 >= 0.5:\n",
    "        assessment = \"GOOD - Competitive with zero-shot approaches\"\n",
    "    elif avg_f1 >= 0.4:\n",
    "        assessment = \"ACCEPTABLE - Baseline performance\"\n",
    "    elif avg_f1 >= 0.3:\n",
    "        assessment = \"BELOW EXPECTATIONS - Needs improvement\"\n",
    "    else:\n",
    "        assessment = \"POOR - System requires significant debugging\"\n",
    "    \n",
    "    print(f\"\\nOVERALL ASSESSMENT (F1={avg_f1:.3f}): {assessment}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
